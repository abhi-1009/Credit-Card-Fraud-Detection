{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fce5bea-329d-4ffb-bb5a-b781ca2bd979",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192b87e1-be0c-4684-bfd8-25c466846c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "684ef329-3f4d-4c29-a2c8-3b6c69eac99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.utils import logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68c26353-6503-4aae-a66f-06f5de486e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c461e24ba01b4e5fb104500e191c5512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8392883bbb1d44d09ef2c040e924b76b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aad9195dcf449a0bdedee9a7825d693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb4cb005bfc448f9f6a5c88a1f1de08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1369f8d15e1d47d184e391445f565a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.75G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b8a37db0394e45a1cbdec84b76fc24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:01<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Disable warnings about padding_side that cannot be rectified with current software:\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "model_name = \"microsoft/DialoGPT-large\"          \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) # , padding_side='left')\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ebd4064-668a-4a00-b85a-356775ce96e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The chat function: received a user input and chat-history and returns the model's reply and chat-history:\n",
    "def reply(input_text, history=None):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input_text + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([history, new_user_input_ids], dim=-1) if history is not None else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    return tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True), chat_history_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e5a375-ee39-4ce7-bd80-14062d66a03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please press enter (not SHIFT-enter) after your input:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  Good morning, I am Peter, and I am doing well.\n"
     ]
    }
   ],
   "source": [
    "history = None\n",
    "first = True\n",
    "while True:\n",
    "    if first is True:\n",
    "        first = False\n",
    "        print(\"Please press enter (not SHIFT-enter) after your input:\")\n",
    "    input_text = input(\"> \")\n",
    "    if input_text in [\"\", \"bye\", \"quit\", \"exit\"]:\n",
    "        break\n",
    "    reply_text, history_new = reply(input_text, history)\n",
    "    history=history_new\n",
    "    if history.shape[1]>80:\n",
    "        old_shape = history.shape\n",
    "        history = history[:,-80:]\n",
    "        print(f\"History cut from {old_shape} to {history.shape}\")\n",
    "    # history_text = tokenizer.decode(history[0])\n",
    "    # print(f\"Current history: {history_text}\")\n",
    "    print(f\"D_GPT: {reply_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66285025-54d5-4b99-889b-5e91ef2979b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
